```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(mia)
library(miaViz)
library(dplyr)
library(stringr)

mae <- microbiomeDataSets::HintikkaXOData()

# Drop off those bacteria that do not include information in Phylum or lower levels
mae[[1]] <- mae[[1]][!is.na(rowData(mae[[1]])$Phylum), ]

# Clean taxonomy data, so that names do not include addtional characters
rowData(mae[[1]]) <- DataFrame(apply(rowData(mae[[1]]), 2, 
                                     str_remove, pattern = "._[0-9]__"))

# For simplicity, classify all high-fat diets as high-fat, and all the low-fat 
# diets as low-fat diets
colData(mae)$Diet <- ifelse(colData(mae)$Diet == "High-fat" | 
                              colData(mae)$Diet == "High-fat + XOS", 
                            "High-fat", "Low-fat")

# Calculates relative abundances, and stores the table to assays
mae[[1]] <- transformCounts(mae[[1]], method = "relabundance")

# Earlier processing
# Threshold: metabolites whose (cv > +threshold or cv < -threshold), will be included
cv_threshold <- 0.5
metabolite_trans <- "nmr"

# Get the data
metabolite_tse <- mae[[2]]

# Calculate coeffieicnt of variation of individual metabolites
df <- data.frame(cv = apply(assay(metabolite_tse, metabolite_trans), 1, 
                            function(x){sd(x)/mean(x)}))

# Get those metabolites that are over threshold
metabolites_over_th <- rownames(df[df$cv > cv_threshold | 
                                     df$cv < -cv_threshold, , drop = FALSE])
# Ignore those metabolites that do not have name / are NA
metabolites_over_th <- metabolites_over_th[!str_detect(metabolites_over_th, "NA")]

rank <- "Genus"
prevalence <- 0.2
detection <- 0.001
taxa_trans <-  "rclr"

# Get bacterial data
taxa_tse <- mae[[1]]
# Agglomerate at Genus level
taxa_tse <- agglomerateByRank(taxa_tse, rank = rank)
# Do CLR transformation
taxa_tse <- transformSamples(taxa_tse, method = "rclr", pseudocount = 1)

# Subset metabolite data
metabolite_tse <- metabolite_tse[metabolites_over_th, ]

# Subset bacterial data by its prevalence. Bacteria whose prevalences are over 
# threshold are included
taxa_tse <- subsetByPrevalentTaxa(taxa_tse, 
                                  prevalence = prevalence, 
                                  detection = detection)

# Remove uncultured and ambiguous(as it's hard to interpret their results)
taxa_tse <- taxa_tse[-grep("uncultured|Ambiguous_taxa", names(taxa_tse)),]

# Define data sets to cross-correlate
x <- t(assay(taxa_tse, taxa_trans))
y <- t(assay(metabolite_tse, "nmr"))
# If there are duplicated taxa names, makes them unique
colnames(x) <- str_remove(colnames(x), paste0(rank, ":"))
colnames(x) <- make.unique(colnames(x))
```

# Supervised learning

Machine learning models are highly flexible and can be used to model differences in 
samples, similarly to (frequentist) statistics. However, the analysis workflow with 
these methods is very different from the frequentist analyses. These models learn 
a function to predict values of the dependent variable, given data. Between different 
models, the algorithms vary greatly, but generally all regression and classification 
models can be used similarly in a machine learning workflow.

Machine learning models do not usually output p-values, but they are designed to 
predict the outcome (value or class) of the dependent variable based on data. 
Thus, if we want to know how good our model is, we need to divide our data to training
and test (or validation) sets. The training set is used to train the model, and the 
validation set can be then used to test the model. The model can be used to predict 
the outcome of the dependent variable on the test data, and the predicted values can
be compared to the actual known values in the test data. It is important to make sure
that there is no data leakage between these two sets, or otherwise the validation is
compromised.

In the workshop we use random forests and the caret package to train regression and 
classification models. The models will predict continuous butyrate concentration or discretized 
class (high/low butyrate) based on the microbiome composition ([why butyrate?](https://scholar.google.com/scholar?as_sdt=0%2C5&as_ylo=2015&q=butyrate+and+gut+microbiome&btnG=)).

## Data curation

We first make a data frame which includes only the butyrate concentration and the 
transformed genus-level microbiome data.

Creating a dataframe for modeling butyrate levels:

```{r, message=FALSE, warning=FALSE}
butyrate_df <- data.frame(cbind(y, x))
butyrate_df <- butyrate_df[,which(colnames(butyrate_df) %in% c("Butyrate", colnames(x)))]
```

A function in the [caret package](https://topepo.github.io/caret/) is used to divide the data once to 80% train and 20% test (validation) 
sets. This is to prevent data leakage and overestimation of model performance. The 
20% test set is only used to conduct the final validation of the models. The number of 
samples in this case is low (n = 40), but as we can see later on, even 8 samples is 
sufficient for estimation of the performance. Note that the data is stratified to include 
a representative distribution of butyrate concentrations on both sides of the split. 
There is some randomness inherent in the splitting, so set.seed() needs to be used.

```{r, warning=FALSE, message=FALSE}
library(caret)
set.seed(42)
trainIndex <- createDataPartition(butyrate_df$Butyrate, p = .8, list = FALSE, times = 1)
butyrate_df_train <- butyrate_df[trainIndex,]
butyrate_df_test <- butyrate_df[-trainIndex,]
```

## Regression with random forests

Random forests are a common and flexible ensemble learning method, which are a good starting
point when choosing a machine learning model. We are using the [ranger](https://github.com/imbs-hl/ranger) implementation 
of random forests, which runs quite fast compared to its alternatives in R. A wrapper 
train function from caret is used to conduct a 5-fold cross-validation, repeated 5 times 
with random partitions inside the training data (for further reading, [see documentation](https://topepo.github.io/caret/model-training-and-tuning.html)). 
Because of the randomness, seed needs to be set again. **Note that the seed does *not* remain set if you re-run a function with a random component
without calling set.seed() first!** Specific to random forests, we are also using an option to use permutation 
importance.

The train function is [quite complex](https://www.rdocumentation.org/packages/caret/versions/4.47/topics/train), and performs hyperparameter tuning while training the 
model with cross-validation. The final model included in the object is then trained on all 
input data and optimized hyperparameters.

```{r, warning=FALSE, message=FALSE}
set.seed(42)
fitControl <- trainControl(method = "repeatedcv", number = 5, repeats = 5)
rfFit1 <- train(Butyrate ~ ., data = butyrate_df_train, 
                method = "ranger", 
                trControl = fitControl,
                importance = "permutation")
```

Following the training, we can print out the resulting object, which shows details about 
the training. We also print out results of the final model.

```{r, warning=FALSE, message=FALSE}
print(rfFit1)
print(rfFit1$finalModel)
```

We can then compare the metrics produced in training to actual validation metrics. Here,
the final model predicts on unseen test data samples, and metrics are calculated 
against the observed (true) values. We can also plot the predicted values against the 
observed values, and include a line to show how a perfect model would predict.

```{r, warning=FALSE, message=FALSE}
test_predictions <- predict(rfFit1, newdata = butyrate_df_test)
print(postResample(test_predictions, butyrate_df_test$Butyrate))

# Plot predicted vs observed
pred_obs <- data.frame(predicted = test_predictions, observed = butyrate_df_test$Butyrate)
ggplot(data = pred_obs, aes(x=predicted, y=observed)) + geom_point(size = 5, color = "orange") + 
  xlab("Predicted butyrate concentration") + ylab("Observed butyrate concentration") +
  lims(x = c(0,5), y = c(0,5)) +
  geom_abline(linetype = 5, color = "blue", size = 1) # Plot a perfect fit line
```

All models are wrong, but some are useful. Luckily, our model seems to be somewhat useful. 
However, we are not yet done with the model. It is important to examine how our model 
actually works. Even though the random forest can look like a "black box", we can get 
much information out on how it ends up making specific predictions.

Feature importance can be used, quite literally, to see which features are important 
for model predictions. Permutation importance is a metric calculated by shuffling the 
values of individual feature columns. Permutation importance of a feature will be high
if such corrupted data leads to bad predictions of the model. If shuffling a feature 
does not affect the model performance negatively, its permutation importance will be low.

```{r, warning=FALSE, message=FALSE, fig.height=6}
plot(varImp(rfFit1))
```

We can see that some genera are highly important for model predictions. The importance 
values can be highly useful, and these are often used *e.g.,* for feature selection before 
conducting statistical (or other ML) tests. However, if we only conducted this supervised 
machine learning analysis, we would not know which features are positively and which 
ones negatively associated with butyrate levels.

All black box models can however be examined through partial dependence plots. Similarly to
validation, we can again utilize the fact that ML models are great at making new predictions.
While the inner workings of the model are highly complex, we can assume that changing 
the values of an important feature affects the model prediction in a specific way. 
Briefly, partial dependence plots visualize the expected output of the model over the 
range of an individual input feature (up to 3 features). The [pdp package](https://bgreenwell.github.io/pdp/articles/pdp.html)
is a versatile implementation for conducting these analyses in R and works directly on models
fitted with caret::train().

```{r, warning=FALSE, message=FALSE, fig.height=6}
library(patchwork)
library(pdp)
top_features <- rownames(varImp(rfFit1)$importance)[order(varImp(rfFit1)$importance[,"Overall"], decreasing = TRUE)[1:6]]
pd_plots <- list(NULL)
for (feature in 1:length(top_features)) {
  pd_plots[[feature]] <- partial(rfFit1, pred.var = top_features[feature], rug = TRUE) %>% autoplot() + 
    geom_hline(yintercept = mean(butyrate_df_train$Butyrate), linetype = 2, color = "gray") + # Show the mean of the training data as a dashed line
    scale_y_continuous(limits=c(1.5,2.3)) # Harmonize the scale of yhat on all plots
  print(paste0("Partial dependence of ", top_features[feature]))
}
wrap_plots(pd_plots)
```

## Classification with random forests

In addition to regression, random forests can also be used for classification tasks.
While the actual butyrate values might be important for some purposes, it can be 
expected that it is quite hard to model the concentration throughout its observed 
range accurately. As a demonstration, we can also discretize a constant variable, 
and handle the problem as a classification task. Here, we are using the median value 
of butyrate as a cutoff for high and low butyrate groups - which makes this a binary 
classification. Other ways to specify a cutoff for discretization are often more 
justified than just the mean or median. These are usually informed by previous studies 
and results.

```{r, warning=FALSE, message=FALSE}
butyrate_cutoff <- median(butyrate_df_test$Butyrate)
butyrate_df_test_2 <- butyrate_df_test
butyrate_df_train_2 <- butyrate_df_train
butyrate_df_test_2$Butyrate <- as.factor(ifelse(butyrate_df_test_2$Butyrate >= butyrate_cutoff, "High", "Low"))
butyrate_df_train_2$Butyrate <- as.factor(ifelse(butyrate_df_train_2$Butyrate >= butyrate_cutoff, "High", "Low"))
```

Training of the model is very similar to regression, but we want to define two options in the 
trainControl function sent to caret::train(). classProbs = TRUE is used to output classification 
probabilities instead of the classes themselves. This is required for calculating 
ROC-AUC values. Also, we are defining a summaryFunction which is used for evaluation 
and hyperparameter optimization.

```{r, warning=FALSE, message=FALSE}
set.seed(42)
fitControl <- trainControl(method = "repeatedcv", number = 5, repeats = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
rfFit2 <- train(Butyrate ~ ., data = butyrate_df_train_2, 
                method = "ranger", 
                trControl = fitControl,
                importance = "permutation")
```

We then measure performance with test data. Here, we need to construct a data frame with 
specific dimensions as an input for twoClassSummary().

```{r, warning=FALSE, message=FALSE}
test_predictions_2 <- data.frame(obs = butyrate_df_test_2$Butyrate, 
                                 pred = predict(rfFit2, newdata = butyrate_df_test_2), 
                                 predict(rfFit2, newdata = butyrate_df_test_2, type = "prob"))

#Print out the metrics
print(rfFit2)
print(rfFit2$finalModel)
print(twoClassSummary(test_predictions_2, lev = c("High", "Low")))
```

Often just the ROC-AUC value calculated above can suffice, for example for 
model comparisons. However, if class distribution is skewed, area under the precision-recall curve
(AUPRC) should be used instead (see [Fu et al., 2018](https://doi.org/10.1002/bimj.201800148)). 

Here is one way to calculate both with the package precrec and plot the curves.

```{r, warning=FALSE, message=FALSE, fig.height=6}
library(MLmetrics)
library(precrec)
aucs <- evalmod(scores = test_predictions_2$Low, labels = test_predictions_2$obs)
print(aucs)
autoplot(aucs)
```

Finally, we can also extract and plot the feature importance from the binary
classification model. It is interesting to compare this result to the importances 
of the same features in the regression model.

```{r, warning=FALSE, message=FALSE, fig.height=6}
plot(varImp(rfFit2))
```
