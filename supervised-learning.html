<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Supervised learning | Introduction to multi-omics data analysis</title>
  <meta name="description" content="Workshop material" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Supervised learning | Introduction to multi-omics data analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Workshop material" />
  <meta name="github-repo" content="microbiome/course_2022_FindingPheno" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Supervised learning | Introduction to multi-omics data analysis" />
  
  <meta name="twitter:description" content="Workshop material" />
  

<meta name="author" content="University of Turku" />


<meta name="date" content="2022-01-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="cross-correlation-unsupervised-learning.html"/>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to multiomic data analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#learning-goals"><i class="fa fa-check"></i><b>1.2</b> Learning goals</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i><b>1.3</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="program.html"><a href="program.html"><i class="fa fa-check"></i><b>2</b> Program</a>
<ul>
<li class="chapter" data-level="2.1" data-path="program.html"><a href="program.html#day-1"><i class="fa fa-check"></i><b>2.1</b> Day 1</a></li>
<li class="chapter" data-level="2.2" data-path="program.html"><a href="program.html#day-2"><i class="fa fa-check"></i><b>2.2</b> Day 2</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>3</b> Getting started</a>
<ul>
<li class="chapter" data-level="3.1" data-path="getting-started.html"><a href="getting-started.html#checklist-before-the-workshop"><i class="fa fa-check"></i><b>3.1</b> Checklist (before the workshop)</a></li>
<li class="chapter" data-level="3.2" data-path="getting-started.html"><a href="getting-started.html#support-and-resources"><i class="fa fa-check"></i><b>3.2</b> Support and resources</a></li>
<li class="chapter" data-level="3.3" data-path="getting-started.html"><a href="getting-started.html#installing-and-loading-the-required-r-packages"><i class="fa fa-check"></i><b>3.3</b> Installing and loading the required R packages</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>4</b> Data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="data.html"><a href="data.html#data-structure"><i class="fa fa-check"></i><b>4.1</b> Data structure</a></li>
<li class="chapter" data-level="4.2" data-path="data.html"><a href="data.html#example-data"><i class="fa fa-check"></i><b>4.2</b> Example data</a></li>
<li class="chapter" data-level="4.3" data-path="data.html"><a href="data.html#importing-data-in-r"><i class="fa fa-check"></i><b>4.3</b> Importing data in R</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="microbiome-data-exploration.html"><a href="microbiome-data-exploration.html"><i class="fa fa-check"></i><b>5</b> Microbiome data exploration</a>
<ul>
<li class="chapter" data-level="5.1" data-path="microbiome-data-exploration.html"><a href="microbiome-data-exploration.html#data-structure-1"><i class="fa fa-check"></i><b>5.1</b> Data structure</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="microbiome-data-exploration.html"><a href="microbiome-data-exploration.html#transformations"><i class="fa fa-check"></i><b>5.1.1</b> Transformations</a></li>
<li class="chapter" data-level="5.1.2" data-path="microbiome-data-exploration.html"><a href="microbiome-data-exploration.html#aggregation"><i class="fa fa-check"></i><b>5.1.2</b> Aggregation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="microbiome-data-exploration.html"><a href="microbiome-data-exploration.html#visualization"><i class="fa fa-check"></i><b>5.2</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="beta-diversity.html"><a href="beta-diversity.html"><i class="fa fa-check"></i><b>6</b> Beta diversity</a>
<ul>
<li class="chapter" data-level="6.1" data-path="beta-diversity.html"><a href="beta-diversity.html#examples-of-pcoa-with-different-settings"><i class="fa fa-check"></i><b>6.1</b> Examples of PCoA with different settings</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="beta-diversity.html"><a href="beta-diversity.html#pcoa-for-asv-level-data-with-bray-curtis"><i class="fa fa-check"></i><b>6.1.1</b> PCoA for ASV-level data with Bray-Curtis</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="beta-diversity.html"><a href="beta-diversity.html#highlighting-external-variables"><i class="fa fa-check"></i><b>6.2</b> Highlighting external variables</a></li>
<li class="chapter" data-level="6.3" data-path="beta-diversity.html"><a href="beta-diversity.html#estimating-associations-with-an-external-variable"><i class="fa fa-check"></i><b>6.3</b> Estimating associations with an external variable</a></li>
<li class="chapter" data-level="6.4" data-path="beta-diversity.html"><a href="beta-diversity.html#community-typing"><i class="fa fa-check"></i><b>6.4</b> Community typing</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="cross-correlation-unsupervised-learning.html"><a href="cross-correlation-unsupervised-learning.html"><i class="fa fa-check"></i><b>7</b> Cross-correlation &amp; Unsupervised learning</a>
<ul>
<li class="chapter" data-level="7.1" data-path="cross-correlation-unsupervised-learning.html"><a href="cross-correlation-unsupervised-learning.html#cross-correlation"><i class="fa fa-check"></i><b>7.1</b> Cross-correlation</a></li>
<li class="chapter" data-level="7.2" data-path="cross-correlation-unsupervised-learning.html"><a href="cross-correlation-unsupervised-learning.html#unsupervised-learning"><i class="fa fa-check"></i><b>7.2</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="cross-correlation-unsupervised-learning.html"><a href="cross-correlation-unsupervised-learning.html#biclustering"><i class="fa fa-check"></i><b>7.2.1</b> Biclustering</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>8</b> Supervised learning</a>
<ul>
<li class="chapter" data-level="8.1" data-path="supervised-learning.html"><a href="supervised-learning.html#data-curation"><i class="fa fa-check"></i><b>8.1</b> Data curation</a></li>
<li class="chapter" data-level="8.2" data-path="supervised-learning.html"><a href="supervised-learning.html#regression-with-random-forests"><i class="fa fa-check"></i><b>8.2</b> Regression with random forests</a></li>
<li class="chapter" data-level="8.3" data-path="supervised-learning.html"><a href="supervised-learning.html#classification-with-random-forests"><i class="fa fa-check"></i><b>8.3</b> Classification with random forests</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to multi-omics data analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="supervised-learning" class="section level1" number="8">
<h1><span class="header-section-number">Chapter 8</span> Supervised learning</h1>
<p>Machine learning models are highly flexible and can be used to model differences in
samples, similarly to (frequentist) statistics. However, the analysis workflow with
these methods is very different from the frequentist analyses. These models learn
a function to predict values of the dependent variable, given data. Between different
models, the algorithms vary greatly, but generally all regression and classification
models can be used similarly in a machine learning workflow.</p>
<p>Machine learning models do not usually output p-values, but they are designed to
predict the outcome (value or class) of the dependent variable based on data.
Thus, if we want to know how good our model is, we need to divide our data to training
and test (or validation) sets. The training set is used to train the model, and the
validation set can be then used to test the model. The model can be used to predict
the outcome of the dependent variable on the test data, and the predicted values can
be compared to the actual known values in the test data. It is important to make sure
that there is no data leakage between these two sets, or otherwise the validation is
compromised.</p>
<p>In the workshop we use random forests and the caret package to train regression and
classification models. The models will predict continuous butyrate concentration or discretized
class (high/low butyrate) based on the microbiome composition (<a href="https://scholar.google.com/scholar?as_sdt=0%2C5&amp;as_ylo=2015&amp;q=butyrate+and+gut+microbiome&amp;btnG=">why butyrate?</a>).</p>
<div id="data-curation" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Data curation</h2>
<p>We first make a data frame which includes only the butyrate concentration and the
transformed genus-level microbiome data.</p>
<p>Creating a dataframe for modeling butyrate levels:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="supervised-learning.html#cb44-1" aria-hidden="true" tabindex="-1"></a>butyrate_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">cbind</span>(y, x))</span>
<span id="cb44-2"><a href="supervised-learning.html#cb44-2" aria-hidden="true" tabindex="-1"></a>butyrate_df <span class="ot">&lt;-</span> butyrate_df[,<span class="fu">which</span>(<span class="fu">colnames</span>(butyrate_df) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;Butyrate&quot;</span>, <span class="fu">colnames</span>(x)))]</span></code></pre></div>
<p>A function in the <a href="https://topepo.github.io/caret/">caret package</a> is used to divide the data once to 80% train and 20% test (validation)
sets. This is to prevent data leakage and overestimation of model performance. The
20% test set is only used to conduct the final validation of the models. The number of
samples in this case is low (n = 40), but as we can see later on, even 8 samples is
sufficient for estimation of the performance. Note that the data is stratified to include
a representative distribution of butyrate concentrations on both sides of the split.
There is some randomness inherent in the splitting, so set.seed() needs to be used.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="supervised-learning.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb45-2"><a href="supervised-learning.html#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb45-3"><a href="supervised-learning.html#cb45-3" aria-hidden="true" tabindex="-1"></a>trainIndex <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(butyrate_df<span class="sc">$</span>Butyrate, <span class="at">p =</span> .<span class="dv">8</span>, <span class="at">list =</span> <span class="cn">FALSE</span>, <span class="at">times =</span> <span class="dv">1</span>)</span>
<span id="cb45-4"><a href="supervised-learning.html#cb45-4" aria-hidden="true" tabindex="-1"></a>butyrate_df_train <span class="ot">&lt;-</span> butyrate_df[trainIndex,]</span>
<span id="cb45-5"><a href="supervised-learning.html#cb45-5" aria-hidden="true" tabindex="-1"></a>butyrate_df_test <span class="ot">&lt;-</span> butyrate_df[<span class="sc">-</span>trainIndex,]</span></code></pre></div>
</div>
<div id="regression-with-random-forests" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Regression with random forests</h2>
<p>Random forests are a common and flexible ensemble learning method, which are a good starting
point when choosing a machine learning model. We are using the <a href="https://github.com/imbs-hl/ranger">ranger</a> implementation
of random forests, which runs quite fast compared to its alternatives in R. A wrapper
train function from caret is used to conduct a 5-fold cross-validation, repeated 5 times
with random partitions inside the training data (for further reading, <a href="https://topepo.github.io/caret/model-training-and-tuning.html">see documentation</a>).
Because of the randomness, seed needs to be set again. <strong>Note that the seed does <em>not</em> remain set if you re-run a function with a random component
without calling set.seed() first!</strong> Specific to random forests, we are also using an option to use permutation
importance.</p>
<p>The train function is <a href="https://www.rdocumentation.org/packages/caret/versions/4.47/topics/train">quite complex</a>, and performs hyperparameter tuning while training the
model with cross-validation. The final model included in the object is then trained on all
input data and optimized hyperparameters.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="supervised-learning.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb46-2"><a href="supervised-learning.html#cb46-2" aria-hidden="true" tabindex="-1"></a>fitControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>, <span class="at">repeats =</span> <span class="dv">5</span>)</span>
<span id="cb46-3"><a href="supervised-learning.html#cb46-3" aria-hidden="true" tabindex="-1"></a>rfFit1 <span class="ot">&lt;-</span> <span class="fu">train</span>(Butyrate <span class="sc">~</span> ., <span class="at">data =</span> butyrate_df_train, </span>
<span id="cb46-4"><a href="supervised-learning.html#cb46-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;ranger&quot;</span>, </span>
<span id="cb46-5"><a href="supervised-learning.html#cb46-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">trControl =</span> fitControl,</span>
<span id="cb46-6"><a href="supervised-learning.html#cb46-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">importance =</span> <span class="st">&quot;permutation&quot;</span>)</span></code></pre></div>
<p>Following the training, we can print out the resulting object, which shows details about
the training. We also print out results of the final model.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="supervised-learning.html#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rfFit1)</span></code></pre></div>
<pre><code>## Random Forest 
## 
## 32 samples
## 28 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold, repeated 5 times) 
## Summary of sample sizes: 24, 26, 25, 28, 25, 25, ... 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   RMSE       Rsquared   MAE      
##    2    variance    0.8801800  0.5870278  0.6802341
##    2    extratrees  0.9039197  0.5888711  0.7113279
##   15    variance    0.8604439  0.5806824  0.6573050
##   15    extratrees  0.8678618  0.5773606  0.6682391
##   28    variance    0.8702166  0.5663926  0.6564932
##   28    extratrees  0.8569955  0.5883242  0.6589236
## 
## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were mtry = 28, splitrule = extratrees
##  and min.node.size = 5.</code></pre>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="supervised-learning.html#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rfFit1<span class="sc">$</span>finalModel)</span></code></pre></div>
<pre><code>## Ranger result
## 
## Call:
##  ranger::ranger(dependent.variable.name = &quot;.outcome&quot;, data = x,      mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size,      splitrule = as.character(param$splitrule), write.forest = TRUE,      probability = classProbs, ...) 
## 
## Type:                             Regression 
## Number of trees:                  500 
## Sample size:                      32 
## Number of independent variables:  28 
## Mtry:                             28 
## Target node size:                 5 
## Variable importance mode:         permutation 
## Splitrule:                        extratrees 
## Number of random splits:          1 
## OOB prediction error (MSE):       0.8650469 
## R squared (OOB):                  0.4563803</code></pre>
<p>We can then compare the metrics produced in training to actual validation metrics. Here,
the final model predicts on unseen test data samples, and metrics are calculated
against the observed (true) values. We can also plot the predicted values against the
observed values, and include a line to show how a perfect model would predict.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="supervised-learning.html#cb51-1" aria-hidden="true" tabindex="-1"></a>test_predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(rfFit1, <span class="at">newdata =</span> butyrate_df_test)</span>
<span id="cb51-2"><a href="supervised-learning.html#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">postResample</span>(test_predictions, butyrate_df_test<span class="sc">$</span>Butyrate))</span></code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 1.1129878 0.4973854 0.9576183</code></pre>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="supervised-learning.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot predicted vs observed</span></span>
<span id="cb53-2"><a href="supervised-learning.html#cb53-2" aria-hidden="true" tabindex="-1"></a>pred_obs <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">predicted =</span> test_predictions, <span class="at">observed =</span> butyrate_df_test<span class="sc">$</span>Butyrate)</span>
<span id="cb53-3"><a href="supervised-learning.html#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> pred_obs, <span class="fu">aes</span>(<span class="at">x=</span>predicted, <span class="at">y=</span>observed)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">5</span>, <span class="at">color =</span> <span class="st">&quot;orange&quot;</span>) <span class="sc">+</span> </span>
<span id="cb53-4"><a href="supervised-learning.html#cb53-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Predicted butyrate concentration&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Observed butyrate concentration&quot;</span>) <span class="sc">+</span></span>
<span id="cb53-5"><a href="supervised-learning.html#cb53-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lims</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">5</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">5</span>)) <span class="sc">+</span></span>
<span id="cb53-6"><a href="supervised-learning.html#cb53-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">linetype =</span> <span class="dv">5</span>, <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">size =</span> <span class="dv">1</span>)<span class="sc">+</span> <span class="co"># Plot a perfect fit line</span></span>
<span id="cb53-7"><a href="supervised-learning.html#cb53-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme</span>(<span class="at">panel.border =</span> <span class="fu">element_rect</span>(<span class="at">colour =</span> <span class="st">&quot;black&quot;</span>, <span class="at">fill =</span> <span class="cn">NA</span>),</span>
<span id="cb53-8"><a href="supervised-learning.html#cb53-8" aria-hidden="true" tabindex="-1"></a>                             <span class="at">panel.background =</span> <span class="fu">element_blank</span>())</span></code></pre></div>
<p><img src="08-supML_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>All models are wrong, but some are useful. Luckily, our model seems to be somewhat useful.
However, we are not yet done with the model. It is important to examine how our model
actually works. Even though the random forest can look like a “black box,” we can get
much information out on how it ends up making specific predictions.</p>
<p>Feature importance can be used, quite literally, to see which features are important
for model predictions. Permutation importance is a metric calculated by shuffling the
values of individual feature columns. Permutation importance of a feature will be high
if such corrupted data leads to bad predictions of the model. If shuffling a feature
does not affect the model performance negatively, its permutation importance will be low.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="supervised-learning.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">varImp</span>(rfFit1))</span></code></pre></div>
<p><img src="08-supML_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>We can see that some genera are highly important for model predictions. The importance
values can be highly useful, and these are often used <em>e.g.,</em> for feature selection before
conducting statistical (or other ML) tests. However, if we only conducted this supervised
machine learning analysis, we would not know which features are positively and which
ones negatively associated with butyrate levels.</p>
<p>All black box models can however be examined through partial dependence plots. Similarly to
validation, we can again utilize the fact that ML models are great at making new predictions.
While the inner workings of the model are highly complex, we can assume that changing
the values of an important feature affects the model prediction in a specific way.
Briefly, partial dependence plots visualize the expected output of the model over the
range of an individual input feature (up to 3 features). The <a href="https://bgreenwell.github.io/pdp/articles/pdp.html">pdp package</a>
is a versatile implementation for conducting these analyses in R and works directly on models
fitted with caret::train().</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="supervised-learning.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb55-2"><a href="supervised-learning.html#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pdp)</span>
<span id="cb55-3"><a href="supervised-learning.html#cb55-3" aria-hidden="true" tabindex="-1"></a>top_features <span class="ot">&lt;-</span> <span class="fu">rownames</span>(<span class="fu">varImp</span>(rfFit1)<span class="sc">$</span>importance)[<span class="fu">order</span>(<span class="fu">varImp</span>(rfFit1)<span class="sc">$</span>importance[,<span class="st">&quot;Overall&quot;</span>], <span class="at">decreasing =</span> <span class="cn">TRUE</span>)[<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>]]</span>
<span id="cb55-4"><a href="supervised-learning.html#cb55-4" aria-hidden="true" tabindex="-1"></a>pd_plots <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="cn">NULL</span>)</span>
<span id="cb55-5"><a href="supervised-learning.html#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (feature <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(top_features)) {</span>
<span id="cb55-6"><a href="supervised-learning.html#cb55-6" aria-hidden="true" tabindex="-1"></a>  pd_plots[[feature]] <span class="ot">&lt;-</span> <span class="fu">partial</span>(rfFit1, <span class="at">pred.var =</span> top_features[feature], <span class="at">rug =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> <span class="fu">autoplot</span>() <span class="sc">+</span> </span>
<span id="cb55-7"><a href="supervised-learning.html#cb55-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fu">mean</span>(butyrate_df_train<span class="sc">$</span>Butyrate), <span class="at">linetype =</span> <span class="dv">2</span>, <span class="at">color =</span> <span class="st">&quot;gray&quot;</span>) <span class="sc">+</span> <span class="co"># Show the mean of the training data as a dashed line</span></span>
<span id="cb55-8"><a href="supervised-learning.html#cb55-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">limits=</span><span class="fu">c</span>(<span class="fl">1.5</span>,<span class="fl">2.3</span>)) <span class="sc">+</span> <span class="co"># Harmonize the scale of yhat on all plots</span></span>
<span id="cb55-9"><a href="supervised-learning.html#cb55-9" aria-hidden="true" tabindex="-1"></a>      <span class="fu">theme</span>(<span class="at">panel.border =</span> <span class="fu">element_rect</span>(<span class="at">colour =</span> <span class="st">&quot;black&quot;</span>, <span class="at">fill =</span> <span class="cn">NA</span>),</span>
<span id="cb55-10"><a href="supervised-learning.html#cb55-10" aria-hidden="true" tabindex="-1"></a>                             <span class="at">panel.background =</span> <span class="fu">element_blank</span>())</span>
<span id="cb55-11"><a href="supervised-learning.html#cb55-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Partial dependence of &quot;</span>, top_features[feature]))</span>
<span id="cb55-12"><a href="supervised-learning.html#cb55-12" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## [1] &quot;Partial dependence of UBA1819&quot;
## [1] &quot;Partial dependence of Faecalitalea&quot;
## [1] &quot;Partial dependence of Papillibacter&quot;
## [1] &quot;Partial dependence of Anaerostipes&quot;
## [1] &quot;Partial dependence of Lachnoclostridium&quot;
## [1] &quot;Partial dependence of Ruminiclostridium&quot;</code></pre>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="supervised-learning.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">wrap_plots</span>(pd_plots)</span></code></pre></div>
<p><img src="08-supML_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="classification-with-random-forests" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Classification with random forests</h2>
<p>In addition to regression, random forests can also be used for classification tasks.
While the actual butyrate values might be important for some purposes, it can be
expected that it is quite hard to model the concentration throughout its observed
range accurately. As a demonstration, we can also discretize a constant variable,
and handle the problem as a classification task. Here, we are using the median value
of butyrate as a cutoff for high and low butyrate groups - which makes this a binary
classification. Other ways to specify a cutoff for discretization are often more
justified than just the mean or median. These are usually informed by previous studies
and results.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="supervised-learning.html#cb58-1" aria-hidden="true" tabindex="-1"></a>butyrate_cutoff <span class="ot">&lt;-</span> <span class="fu">median</span>(butyrate_df_test<span class="sc">$</span>Butyrate)</span>
<span id="cb58-2"><a href="supervised-learning.html#cb58-2" aria-hidden="true" tabindex="-1"></a>butyrate_df_test_2 <span class="ot">&lt;-</span> butyrate_df_test</span>
<span id="cb58-3"><a href="supervised-learning.html#cb58-3" aria-hidden="true" tabindex="-1"></a>butyrate_df_train_2 <span class="ot">&lt;-</span> butyrate_df_train</span>
<span id="cb58-4"><a href="supervised-learning.html#cb58-4" aria-hidden="true" tabindex="-1"></a>butyrate_df_test_2<span class="sc">$</span>Butyrate <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(butyrate_df_test_2<span class="sc">$</span>Butyrate <span class="sc">&gt;=</span> butyrate_cutoff, <span class="st">&quot;High&quot;</span>, <span class="st">&quot;Low&quot;</span>))</span>
<span id="cb58-5"><a href="supervised-learning.html#cb58-5" aria-hidden="true" tabindex="-1"></a>butyrate_df_train_2<span class="sc">$</span>Butyrate <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(butyrate_df_train_2<span class="sc">$</span>Butyrate <span class="sc">&gt;=</span> butyrate_cutoff, <span class="st">&quot;High&quot;</span>, <span class="st">&quot;Low&quot;</span>))</span></code></pre></div>
<p>Training of the model is very similar to regression, but we want to define two options in the
trainControl function sent to caret::train(). classProbs = TRUE is used to output classification
probabilities instead of the classes themselves. This is required for calculating
ROC-AUC values. Also, we are defining a summaryFunction which is used for evaluation
and hyperparameter optimization.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="supervised-learning.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb59-2"><a href="supervised-learning.html#cb59-2" aria-hidden="true" tabindex="-1"></a>fitControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>, <span class="at">repeats =</span> <span class="dv">10</span>, <span class="at">classProbs =</span> <span class="cn">TRUE</span>, <span class="at">summaryFunction =</span> twoClassSummary)</span>
<span id="cb59-3"><a href="supervised-learning.html#cb59-3" aria-hidden="true" tabindex="-1"></a>rfFit2 <span class="ot">&lt;-</span> <span class="fu">train</span>(Butyrate <span class="sc">~</span> ., <span class="at">data =</span> butyrate_df_train_2, </span>
<span id="cb59-4"><a href="supervised-learning.html#cb59-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;ranger&quot;</span>, </span>
<span id="cb59-5"><a href="supervised-learning.html#cb59-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">trControl =</span> fitControl,</span>
<span id="cb59-6"><a href="supervised-learning.html#cb59-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">importance =</span> <span class="st">&quot;permutation&quot;</span>)</span></code></pre></div>
<p>We then measure performance with test data. Here, we need to construct a data frame with
specific dimensions as an input for twoClassSummary().</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="supervised-learning.html#cb60-1" aria-hidden="true" tabindex="-1"></a>test_predictions_2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">obs =</span> butyrate_df_test_2<span class="sc">$</span>Butyrate, </span>
<span id="cb60-2"><a href="supervised-learning.html#cb60-2" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">pred =</span> <span class="fu">predict</span>(rfFit2, <span class="at">newdata =</span> butyrate_df_test_2), </span>
<span id="cb60-3"><a href="supervised-learning.html#cb60-3" aria-hidden="true" tabindex="-1"></a>                                 <span class="fu">predict</span>(rfFit2, <span class="at">newdata =</span> butyrate_df_test_2, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>))</span>
<span id="cb60-4"><a href="supervised-learning.html#cb60-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-5"><a href="supervised-learning.html#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Print out the metrics</span></span>
<span id="cb60-6"><a href="supervised-learning.html#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rfFit2)</span></code></pre></div>
<pre><code>## Random Forest 
## 
## 32 samples
## 28 predictors
##  2 classes: &#39;High&#39;, &#39;Low&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold, repeated 10 times) 
## Summary of sample sizes: 25, 25, 26, 26, 26, 26, ... 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   ROC        Sens       Spec     
##    2    gini        0.8622222  0.7733333  0.8433333
##    2    extratrees  0.8877778  0.8133333  0.8450000
##   15    gini        0.8261111  0.7866667  0.7950000
##   15    extratrees  0.8561111  0.8266667  0.8116667
##   28    gini        0.8150000  0.7733333  0.7950000
##   28    extratrees  0.8488889  0.8200000  0.8016667
## 
## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 1
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were mtry = 2, splitrule = extratrees
##  and min.node.size = 1.</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="supervised-learning.html#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rfFit2<span class="sc">$</span>finalModel)</span></code></pre></div>
<pre><code>## Ranger result
## 
## Call:
##  ranger::ranger(dependent.variable.name = &quot;.outcome&quot;, data = x,      mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size,      splitrule = as.character(param$splitrule), write.forest = TRUE,      probability = classProbs, ...) 
## 
## Type:                             Probability estimation 
## Number of trees:                  500 
## Sample size:                      32 
## Number of independent variables:  28 
## Mtry:                             2 
## Target node size:                 1 
## Variable importance mode:         permutation 
## Splitrule:                        extratrees 
## Number of random splits:          1 
## OOB prediction error (Brier s.):  0.163163</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="supervised-learning.html#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">twoClassSummary</span>(test_predictions_2, <span class="at">lev =</span> <span class="fu">c</span>(<span class="st">&quot;High&quot;</span>, <span class="st">&quot;Low&quot;</span>)))</span></code></pre></div>
<pre><code>##   ROC  Sens  Spec 
## 0.875 0.750 0.750</code></pre>
<p>Often just the ROC-AUC value calculated above can suffice, for example for
model comparisons. However, if class distribution is skewed, area under the precision-recall curve
(AUPRC) should be used instead (see <a href="https://doi.org/10.1002/bimj.201800148">Fu et al., 2018</a>).</p>
<p>Here is one way to calculate both with the package precrec and plot the curves.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="supervised-learning.html#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MLmetrics)</span>
<span id="cb66-2"><a href="supervised-learning.html#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(precrec)</span>
<span id="cb66-3"><a href="supervised-learning.html#cb66-3" aria-hidden="true" tabindex="-1"></a>aucs <span class="ot">&lt;-</span> <span class="fu">evalmod</span>(<span class="at">scores =</span> test_predictions_2<span class="sc">$</span>Low, <span class="at">labels =</span> test_predictions_2<span class="sc">$</span>obs)</span>
<span id="cb66-4"><a href="supervised-learning.html#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(aucs)</span></code></pre></div>
<pre><code>## 
##     === AUCs ===
## 
##      Model name Dataset ID Curve type       AUC
##    1         m1          1        ROC 0.8750000
##    2         m1          1        PRC 0.8722936
## 
## 
##     === Input data ===
## 
##      Model name Dataset ID # of negatives # of positives
##    1         m1          1              4              4</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="supervised-learning.html#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(aucs)</span></code></pre></div>
<p><img src="08-supML_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Finally, we can also extract and plot the feature importance from the binary
classification model. It is interesting to compare this result to the importances
of the same features in the regression model.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="supervised-learning.html#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">varImp</span>(rfFit2))</span></code></pre></div>
<p><img src="08-supML_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>

<div id="refs" class="references csl-bib-body hanging-indent">
<div class="csl-entry">
Borman, Tuomas, Henrik Eckermann, Chouaib Benchraka, Matti Ruuskanen, and Leo Lahti. 2022. <em>Introduction to Multi-Omics Data Analysis</em>. <a href="https://microbiome.github.io/course_2022_FindingPheno/">microbiome.github.io/course_2022_FindingPheno/</a>.
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="cross-correlation-unsupervised-learning.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["FindingPheno2022_material.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
